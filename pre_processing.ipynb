{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from issue import Issue\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DATA_FILE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../vscode_issues_SA.csv.gzip\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# \"../teste.csv.gzip\" # \"../vscode_issues_SA.csv.gzip\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sample_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(DATA_FILE_PATH, compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m'\u001b[39m, lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m issues_dict_list \u001b[38;5;241m=\u001b[39m sample_dataset\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m issues \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "DATA_FILE_PATH = \"../vscode_issues_SA.csv.gzip\" # \"../teste.csv.gzip\" # \"../vscode_issues_SA.csv.gzip\"\n",
    "\n",
    "sample_dataset = pd.read_csv(DATA_FILE_PATH, compression='gzip', lineterminator='\\n')\n",
    "\n",
    "issues_dict_list = sample_dataset.to_dict('records')\n",
    "\n",
    "issues = []\n",
    "test_dataset = []\n",
    "\n",
    "for issue_dict in tqdm(issues_dict_list):\n",
    "\ttry:\n",
    "\t\tnew_issue = Issue.from_dict(issue_dict)\n",
    "\t\tissues.append(issues)\n",
    "\texcept Exception as e:\n",
    "\t\tpass\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7512\n"
     ]
    }
   ],
   "source": [
    "print(len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "def apply_steps_to_dataset(processing_funcs, dataset):\n",
    "\t\"\"\"\n",
    "\tGiven a list of preocessing functions and a dataset (list of issues), \n",
    "\tapplies each function to the dataset in the order given.\n",
    "\n",
    "\tEach function must return the altered issue, unless they are\n",
    "\tsupposed to be filtered out, in which case the function \n",
    "\tmust return None.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tnew_issues = []\n",
    "\n",
    "\tfor issue in dataset:\n",
    "\t\tfor func in processing_funcs:\n",
    "\t\t\tissue = func(issue)\n",
    "\t\t\tif issue is None:\n",
    "\t\t\t\tbreak\n",
    "\t\tif issue is not None:\n",
    "\t\t\tnew_issues.append(issue)\n",
    "\t\n",
    "\tprint(\"New dataset has \" + len(new_issues) + \" issues\")\n",
    "\n",
    "\treturn new_issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_test_dataset(issue):\n",
    "\t\"\"\"\n",
    "\tChecks the issue id. If it is in the range of the test set (210000 < id <= 220000),\n",
    "\treturn the issue. Otherwise, return None.\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_main_training_dataset(issue):\n",
    "\t\"\"\"\n",
    "\tChecks the issue id. If it is in the range of the larger training set (id <= 210000),\n",
    "\treturn the issue. Otherwise, return None.\n",
    "\t\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_recent_issues_training_dataset(issue):\n",
    "\t\"\"\"\n",
    "\tChecks the issue id. If it is in the range of the training set which only contains\n",
    "\trecent issues (190000<= id <= 210000), return the issue. Otherwise, return None.\n",
    "\t\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_basic_trainingset_requirements(issue):\n",
    "\t\"\"\"\n",
    "\tChecks if a given issue corresponds to the basic requirements for the\n",
    "\ttraining set are met These are vscode's issues that \n",
    "\t\t(i) are closed; \n",
    "\t\t(ii) have exactly one assignee;\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unfrequent_commiters(issue):\n",
    "\t\"\"\"\n",
    "\tUses the code/technique from the `contribution_counter` notebook to filter out\n",
    "\treviews from auhors with \n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_issue_title(issue):\n",
    "\t\"\"\"\n",
    "\tCleans the issue field of the given issue.\n",
    "\t\"\"\"\n",
    "\tnew_title = issue.summary\n",
    "\n",
    "\t# Remove mention to other issues\n",
    "\tnew_title = re.sub(r\"\\[?\\s*[Ff]ollow up to #?[\\d]+\\s*\\]?\", \"\", new_title)\n",
    "\n",
    "\t# Remove monospacing markdown formatting\n",
    "\tnew_title = re.sub(r\"`([\\s\\S]*?)`\", r\"\\1\", new_title)\n",
    "\n",
    "\treturn new_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BE CAREFUL IF ALTERING THESE CONSTANTS.\n",
    "# They should be the same used for training the model in the training notbook (.ipynb)\n",
    "CODE_BEGIN_SENTINEL = \"<BoC>\"\n",
    "CODE_END_SENTINEL = \"<EoC>\"\n",
    "\n",
    "def clean_issue_body(issue):\n",
    "\t\"\"\"\n",
    "\tCleans the body field of the given issue.\n",
    "\tAdditionally, envolves code fragments using the sentinel tokens\n",
    "\tfrom the training notebook.\n",
    "\t\"\"\"\n",
    "\n",
    "\tissue_body = issue.body\n",
    "\n",
    "\tif issue_body is None:\n",
    "\t\treturn \"\"\n",
    "\n",
    "\tcode_fragments = \"\"\n",
    "\n",
    "\tnew_body = issue_body\n",
    "\n",
    "\t#  TODO: Dont just isolate all fragments, but preserve\n",
    "\t# their place in the issue body, surrounding them with \n",
    "\t# the sentinel tokens\n",
    "\n",
    "\t## Note code fragments\n",
    "\tfor match in re.findall(r\"```([\\s\\S]*?)```\", new_body):\n",
    "\t\tcode_fragments += CODE_BEGIN_SENTINEL + match + CODE_END_SENTINEL + \"\\n\"\n",
    "\n",
    "\tnew_body = re.sub(r\"```([\\s\\S]*?)```\", \"\", new_body)\n",
    "\n",
    "\t# Remove headers\n",
    "\tnew_body = re.sub(\"#+ \", \"\", new_body)\n",
    "\n",
    "\t# Removing emphasis might interfere with code fragments\n",
    "\t# Watchout if you want to fill the TODO, as these lines\n",
    "\t# will have to be taken care of\n",
    "\n",
    "\tnew_body = re.sub(r\"_([\\s\\S]*?)_\", r\"\\1\", new_body)\n",
    "\tnew_body = re.sub(r\"\\*([\\s\\S]*?)\\*\", r\"\\1\", new_body)\n",
    "\tnew_body = re.sub(r\"`(\\s[\\S]*?)`\", r\"\\1\", new_body)\n",
    "\n",
    "\t# Remove html tags\n",
    "\tnew_body = re.sub(r\"<[\\s\\S]*?>\", r\"\", new_body)\n",
    "\n",
    "\t# Remove markdown links\n",
    "\tnew_body = re.sub(r\"\\!?\\[[\\s\\S]+\\]\\([\\S]+\\)\", \"\", new_body)\n",
    "\n",
    "\t# Remove attachments\n",
    "\tnew_body = re.sub(r\"https?://[\\S]+\", \"\", new_body)\n",
    "\n",
    "\tnew_body = re.sub(r\"[\\s]*\\n+\", \"\\n\", new_body)\n",
    "\n",
    "\tnew_body = code_fragments + new_body\n",
    "\n",
    "\treturn new_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Issue Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataset = apply_steps_to_dataset([filter_basic_trainingset_requirements,\\\n",
    "\t\t\t\t\t\t\t\t\t\tfilter_unfrequent_commiters,\\\n",
    "\t\t\t\t\t\t\t\t\t\tclean_issue_title,\\\n",
    "\t\t\t\t\t\t\t\t\t\tclean_issue_body],issues)\n",
    "\n",
    "main_training_dataset = apply_steps_to_dataset([filter_main_training_dataset],clean_dataset)\n",
    "recent_issues_training_dataset = apply_steps_to_dataset([filter_recent_issues_training_dataset],clean_dataset)\n",
    "test_dataset = apply_steps_to_dataset([filter_test_dataset],clean_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_issue_repo(new_path,issue_repo):\n",
    "\tissues_as_dicts = []\n",
    "\tprint(\"Parsing collected issues.\\nThis might take a few minutes\")\n",
    "\n",
    "\tfor issue in tqdm(issue_repo):\n",
    "\t\t# print(vars(issue))\n",
    "\t\n",
    "\t\tissues_as_dicts.append(issue.to_dict())\n",
    "\n",
    "\tissues_as_dataset = pd.DataFrame.from_dict(issues_as_dicts)\n",
    "\tissues_as_dataset.to_csv(new_path, compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_TRAINING_DESTINATION_PATH = \"../train_A.csv.gzip\"\n",
    "RECENT_TRAINING_DESTINATION_PATH = \"../train_B.csv.gzip\"\n",
    "TEST_DESTINATION_PATH = \"../test.csv.gzip\"\n",
    "\n",
    "save_issue_repo(MAIN_TRAINING_DESTINATION_PATH,main_training_dataset)\n",
    "save_issue_repo(RECENT_TRAINING_DESTINATION_PATH,recent_issues_training_dataset)\n",
    "save_issue_repo(TEST_DESTINATION_PATH,test_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
