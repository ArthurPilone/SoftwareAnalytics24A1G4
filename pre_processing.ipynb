{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:26:50.889159Z",
     "start_time": "2024-10-29T21:26:50.886656Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from issue import Issue\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:14.484261Z",
     "start_time": "2024-10-29T21:26:52.405440Z"
    }
   },
   "source": [
    "DATA_FILE_PATH = \"vscode_issues.csv.gzip\" # \"../teste.csv.gzip\" # \"../vscode_issues_SA.csv.gzip\"\n",
    "\n",
    "sample_dataset = pd.read_csv(DATA_FILE_PATH, compression='gzip', lineterminator='\\n')\n",
    "\n",
    "issues_dict_list = sample_dataset.to_dict('records')\n",
    "\n",
    "issues = []\n",
    "test_dataset = []\n",
    "\n",
    "# To recode the number of times each author was assigned as an Assignee.\n",
    "times_as_assignee = {}\n",
    "\n",
    "for issue_dict in tqdm(issues_dict_list):\n",
    "    try:\n",
    "        new_issue = Issue.from_dict(issue_dict)\n",
    "        issues.append(new_issue)\n",
    "\n",
    "        # Later the dictionary will be used to filter out authors who have been assigned as an Assignee too few times.\n",
    "        author = new_issue.assignee\n",
    "        if author:  # Ensure author is not None\n",
    "            if times_as_assignee.get(author) is None:\n",
    "                times_as_assignee[author] = 1\n",
    "            else:\n",
    "                times_as_assignee[author] += 1\n",
    "    except Exception as e:\n",
    "        pass"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184819/184819 [00:18<00:00, 9808.60it/s] \n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:21.562403Z",
     "start_time": "2024-10-29T21:27:21.559942Z"
    }
   },
   "source": [
    "print(len(issues))\n",
    "print(len(times_as_assignee))\n",
    "print(times_as_assignee)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184812\n",
      "116\n",
      "{'mjbvz': 15630, 'rzhao271': 1423, 'lszomoru': 3903, 'roblourens': 9410, 'alexr00': 3423, 'sandy081': 8016, 'aiday-mar': 832, 'meganrogge': 5018, 'deepak1556': 4121, 'Tyriar': 12408, 'jrieken': 9073, 'bpasero': 11117, 'joshspicer': 42, 'justschen': 389, 'benibenj': 601, 'andreamah': 1163, 'alexdima': 7050, 'ulugbekna': 431, 'hediet': 2520, 'chrmarti': 1819, 'joaomoreno': 9579, 'connor4312': 2725, 'lramos15': 1832, 'TylerLeonhardt': 2089, 'karthiknadig': 25, 'rebornix': 5550, 'bhavyaus': 406, 'aeschli': 7067, 'Yoyokrazy': 302, 'eleanorjboyd': 32, 'amunger': 323, 'anthonykim1': 10, 'sbatten': 2510, 'joyceerhl': 479, 'isidorn': 7503, 'daviddossett': 338, 'DonJayamanne': 280, 'dbaeumer': 1879, 'bamurtaugh': 10, 'ntrogh': 3, 'chrisdias': 293, 'kieferrm': 314, 'hbons': 86, 'karrtikr': 15, 'cwebster-99': 1, 'digitarald': 79, 'brettcannon': 10, 'MeghanKulkarni': 17, 'paulacamargo25': 7, 'gregvanl': 102, 'danyeh': 122, 'esonnino': 7, 'csigs': 5, 'Chuxel': 6, 'weinand': 2712, 'egamma': 774, 'DanielRosenwasser': 1, 'stevencl': 126, 'IanMatthewHuff': 5, 'tanhakabir': 68, 'miguelsolorio': 554, 'devinvalenciano': 4, 'sadasant': 1, 'minsa110': 4, 'rchiodo': 8, 'greazer': 2, 'JacksonKearl': 1175, 'lychung7': 1, 'orta': 1, 'sana-ajani': 16, 'sanket856': 1, 'eamodio': 567, 'dynamicwebpaige': 1, 'manav014': 1, 'stuartleeks': 1, 'xisui-MSFT': 1, 'kimadeline': 1, 'ItalyPaleAle': 1, 'RMacfarlane': 770, 'claudiaregio': 1, 'nexue2020': 7, 'fiveisprime': 24, 'ornelladotcom': 1, 'btholt': 2, '9at8': 6, 'octref': 684, 'janbaltus': 11, 'bowdenk7': 3, 'cleidigh': 45, 'djdakta': 1, 'weeteckt': 1, 'lukaschal': 47, 'auchenberg': 34, 'awvalenti': 1, 'foucdeg': 1, 'Steam-Rabbit': 1, 'ramya-rao-a': 786, 'shawndon': 2, 'albu77': 1, 'gushuro': 10, 'tsalinger': 12, 'seanmcbreen': 69, 'johnliu369': 17, 'daviwil': 15, 'Lixire': 12, 'michelkaporin': 76, 'waderyan': 390, 'aefernandes': 1, 'mousetraps': 20, 'bgashler1': 69, 'vsccarl': 23, 'v-pavanp': 15, 'delmyers': 4, 'sofianhn': 21, 'pjmeyer': 1, 'lukehoban': 1}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:23.402148Z",
     "start_time": "2024-10-29T21:27:23.399146Z"
    }
   },
   "source": [
    "def apply_steps_to_dataset(processing_funcs, dataset):\n",
    "\t\"\"\"\n",
    "\tIterates over each issue in the dataset and applies the provided list of pre_processing functions in the given order.\n",
    "\n",
    "\tEach function must return the altered issue, unless they are\n",
    "\tsupposed to be filtered out, in which case the function \n",
    "\tmust return None.\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tnew_issues = []\n",
    "\n",
    "\tfor issue in dataset:\n",
    "\t\tfor func in processing_funcs:\n",
    "\t\t\tissue = func(issue)\n",
    "\t\t\tif issue is None:\n",
    "\t\t\t\tbreak\n",
    "\t\tif issue is not None:\n",
    "\t\t\tnew_issues.append(issue)\n",
    "\t\n",
    "\tprint(\"New dataset has \" + str(len(new_issues)) + \" issues\")\n",
    "\n",
    "\treturn new_issues"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Steps"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:25.040510Z",
     "start_time": "2024-10-29T21:27:25.038037Z"
    }
   },
   "source": [
    "def filter_test_dataset(issue):\n",
    "\t\"\"\"\n",
    "\tChecks the issue id. If it is in the range of the test set (210000 < id <= 220000),\n",
    "\treturn the issue. Otherwise, return None.\n",
    "\t\"\"\"\n",
    "\tissue_id = int(issue.identifier)\n",
    "\tif 210000 < issue_id <= 220000:\n",
    "\t\treturn issue\n",
    "\treturn None"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:25.973191Z",
     "start_time": "2024-10-29T21:27:25.970430Z"
    }
   },
   "source": [
    "def filter_main_training_dataset(issue):\n",
    "\t\"\"\"\n",
    "\tChecks the issue id. If it is in the range of the larger training set (id <= 210000),\n",
    "\treturn the issue. Otherwise, return None.\n",
    "\t\"\"\"\n",
    "\tissue_id = int(issue.identifier)\n",
    "\tif issue_id <= 210000:\n",
    "\t\treturn issue\n",
    "\treturn None"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:27.156117Z",
     "start_time": "2024-10-29T21:27:27.153293Z"
    }
   },
   "source": [
    "def filter_recent_issues_training_dataset(issue):\n",
    "\t\"\"\"\n",
    "\tChecks the issue id. If it is in the range of the training set which only contains\n",
    "\trecent issues (190000<= id <= 210000), return the issue. Otherwise, return None.\n",
    "\t\"\"\"\n",
    "\tissue_id = int(issue.identifier)\n",
    "\tif 190000 <= issue_id <= 210000:\n",
    "\t\treturn issue\n",
    "\treturn None"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:28.488311Z",
     "start_time": "2024-10-29T21:27:28.485431Z"
    }
   },
   "source": [
    "def filter_basic_trainingset_requirements(issue):\n",
    "\t\"\"\"\n",
    "\tChecks if a given issue corresponds to the basic requirements for the\n",
    "\ttraining set are met. These are vscode's issues that \n",
    "\t\t(i) are closed; \n",
    "\t\t(ii) have exactly one assignee;\n",
    "\t\"\"\"\n",
    "\t# Check if the issue is closed\n",
    "\tif issue.completion_time is None:\n",
    "\t\treturn None\n",
    "\t\n",
    "\t# Check if the issue has exactly one assignee\n",
    "\tif issue.assignee is None or (isinstance(issue.assignee, list) and len(issue.assignee) != 1):\n",
    "\t\treturn None\n",
    "\t\n",
    "\t# If both conditions are met, return the issue\n",
    "\treturn issue"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:29.954523Z",
     "start_time": "2024-10-29T21:27:29.951813Z"
    }
   },
   "source": [
    "def filter_unfrequent_assignees(issue):\n",
    "    \"\"\"\n",
    "    Filters out issues from authors who was as an assignee too few times(lower than the threshold).\n",
    "    \"\"\"\n",
    "    threshold = 1\n",
    "    \n",
    "    author = issue.assignee\n",
    "    \n",
    "    # Check if the number of times each author as assignee is above 1\n",
    "    if author is None or times_as_assignee.get(author, 0) <= threshold:\n",
    "        return None\n",
    "    \n",
    "    return issue"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:31.543076Z",
     "start_time": "2024-10-29T21:27:31.540100Z"
    }
   },
   "source": [
    "def clean_issue_title(issue):\n",
    "\t\"\"\"\n",
    "\tCleans the issue field of the given issue.\n",
    "\t\"\"\"\n",
    "\tnew_title = issue.summary\n",
    "\n",
    "\t# Remove mention to other issues\n",
    "\tnew_title = re.sub(r\"\\[?\\s*[Ff]ollow up to #?[\\d]+\\s*\\]?\", \"\", new_title)\n",
    "\n",
    "\t# Remove monospacing markdown formatting\n",
    "\tnew_title = re.sub(r\"`([\\s\\S]*?)`\", r\"\\1\", new_title)\n",
    "\n",
    "\t# Update the issue summary\n",
    "\tissue.summary = new_title\n",
    "\t\n",
    "\t# Return the updated issue object\n",
    "\treturn issue\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:33.119965Z",
     "start_time": "2024-10-29T21:27:33.115182Z"
    }
   },
   "source": [
    "# Be cautious when changing these constants.\n",
    "# They must match those used in the training notebook (.ipynb).\n",
    "CODE_BEGIN_SENTINEL = \"<BoC>\"\n",
    "CODE_END_SENTINEL = \"<EoC>\"\n",
    "\n",
    "def clean_issue_body(issue):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the body field of the given issue.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Wraps code fragments within sentinel tokens to help the model recognize code blocks.\n",
    "    2. Removes unnecessary formatting elements like headers, emphasis, markdown links,\n",
    "       and HTML tags.\n",
    "    3. Preserves code fragments in their original position within the issue body,\n",
    "       surrounding them with the predefined sentinel tokens for consistency with training.\n",
    "    \"\"\"\n",
    "\n",
    "    # Access the body content of the issue\n",
    "    issue_body = issue.body\n",
    "\n",
    "    # If the issue body is None or empty, return an empty string as the cleaned content\n",
    "    if issue_body is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Make a copy of the original body for processing\n",
    "    new_body = issue_body\n",
    "\n",
    "    # Preserve code fragments in their original position with sentinel tokens.\n",
    "    # Replace each code block (```) with sentinel-wrapped content\n",
    "    new_body = re.sub(r\"```([\\s\\S]*?)```\",\n",
    "                      lambda match: CODE_BEGIN_SENTINEL + match.group(1) + CODE_END_SENTINEL,\n",
    "                      new_body)\n",
    "\n",
    "    # Remove headers (lines starting with one or more # characters)\n",
    "    new_body = re.sub(\"#+ \", \"\", new_body)\n",
    "\n",
    "    # Remove emphasis formatting (italics and bold) by replacing underscores, asterisks, and backticks\n",
    "    # surrounding text while preserving the inner text content.\n",
    "    # Note: Ensure code fragments are wrapped in sentinels before this step, as these substitutions\n",
    "    # could interfere with the original code formatting.\n",
    "    new_body = re.sub(r\"_([\\s\\S]*?)_\", r\"\\1\", new_body)\n",
    "    new_body = re.sub(r\"\\*([\\s\\S]*?)\\*\", r\"\\1\", new_body)\n",
    "    new_body = re.sub(r\"`(\\s[\\S]*?)`\", r\"\\1\", new_body)\n",
    "\n",
    "    # Remove HTML tags, keeping only the text content\n",
    "    new_body = re.sub(r\"<[\\s\\S]*?>\", r\"\", new_body)\n",
    "\n",
    "    # Remove markdown-style links and images, keeping only the text description if available\n",
    "    new_body = re.sub(r\"\\!?$begin:math:display$[\\\\s\\\\S]+$end:math:display$$begin:math:text$[\\\\S]+$end:math:text$\", \"\", new_body)\n",
    "\n",
    "    # Remove any URL attachments, such as external links or images\n",
    "    new_body = re.sub(r\"https?://[\\S]+\", \"\", new_body)\n",
    "\n",
    "    # Clean up excessive blank lines, condensing multiple newlines to a single newline\n",
    "    new_body = re.sub(r\"[\\s]*\\n+\", \"\\n\", new_body)\n",
    "\n",
    "    # Assign the cleaned and processed content back to the issue body\n",
    "    issue.body = new_body\n",
    "\n",
    "    # Return the modified issue object with the updated body\n",
    "    return issue\n"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Issue Datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:47.562262Z",
     "start_time": "2024-10-29T21:27:35.705134Z"
    }
   },
   "source": [
    "clean_dataset = apply_steps_to_dataset([filter_basic_trainingset_requirements,\\\n",
    "\t\t\t\t\t\t\t\t\t\tfilter_unfrequent_assignees,\\\n",
    "\t\t\t\t\t\t\t\t\t\tclean_issue_title,\\\n",
    "\t\t\t\t\t\t\t\t\t\tclean_issue_body],issues)\n",
    "\n",
    "main_training_dataset = apply_steps_to_dataset([filter_main_training_dataset],clean_dataset)\n",
    "recent_issues_training_dataset = apply_steps_to_dataset([filter_recent_issues_training_dataset],clean_dataset)\n",
    "test_dataset = apply_steps_to_dataset([filter_test_dataset],clean_dataset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset has 142391 issues\n",
      "New dataset has 135926 issues\n",
      "New dataset has 10299 issues\n",
      "New dataset has 3135 issues\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the results to a new file"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:49.053855Z",
     "start_time": "2024-10-29T21:27:49.050652Z"
    }
   },
   "source": [
    "def save_issue_repo(new_path,issue_repo):\n",
    "\tissues_as_dicts = []\n",
    "\tprint(\"Parsing collected issues.\\nThis might take a few minutes\")\n",
    "\n",
    "\tfor issue in tqdm(issue_repo):\n",
    "\t\t# print(vars(issue))\n",
    "\t\n",
    "\t\tissues_as_dicts.append(issue.to_dict())\n",
    "\n",
    "\tissues_as_dataset = pd.DataFrame.from_dict(issues_as_dicts)\n",
    "\tissues_as_dataset.to_csv(new_path, compression='gzip', index=False)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:27:57.835921Z",
     "start_time": "2024-10-29T21:27:50.917389Z"
    }
   },
   "source": [
    "MAIN_TRAINING_DESTINATION_PATH = \"train_A.csv.gzip\"\n",
    "RECENT_TRAINING_DESTINATION_PATH = \"train_B.csv.gzip\"\n",
    "TEST_DESTINATION_PATH = \"test.csv.gzip\"\n",
    "\n",
    "save_issue_repo(MAIN_TRAINING_DESTINATION_PATH,main_training_dataset)\n",
    "save_issue_repo(RECENT_TRAINING_DESTINATION_PATH,recent_issues_training_dataset)\n",
    "save_issue_repo(TEST_DESTINATION_PATH,test_dataset)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing collected issues.\n",
      "This might take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135926/135926 [00:00<00:00, 3065810.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing collected issues.\n",
      "This might take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10299/10299 [00:00<00:00, 3116226.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing collected issues.\n",
      "This might take a few minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3135/3135 [00:00<00:00, 2475831.87it/s]\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
